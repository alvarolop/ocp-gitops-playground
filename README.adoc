= Red Hat Openshift GitOps Playground
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2021-10
// Metadata
:description: This document shows how to do a basic installation of ArgoCD customizing basic configuration of login.
:keywords: argocd, gitops, openshift, red hat
// Settings
:toc:
:toclevels: 3
:sectanchors:
:sectnumlevels: 3
:sectnums: 
:source-highlighter: pygments
:imagesdir: images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
ifndef::env-github[]
:icons: font
endif::[]
// End: Enable admonition icons
// Refs
:ocp-cluster-project: gitops
:ocp-cluster-name: argocd-cluster
:ocp-app-project: spring-petclinic



== Introduction

GitOps is an increasingly popular set of practices for managing the complexities of running hybrid multicluster Kubernetes infrastructure. GitOps centers on treating Git repositories as the single source of truth and applying Git workflows that have been consistently used for application development to infrastructure and application operators. 

This repository provides a starting point to deploy the Openshift GitOps operator as well as the first ArgoCD application. In the annex, you can also see how to deploy a Sonatype Nexus Repository to store your CI/CD artifacts such as `.jar`, helm charts, etc.


== Helm repository

This GitHub repository also acts as a Helm Repository that only serves the `argocd-config` Helm Package. If I generate a new version of the chart, I just run the following commands:

[source, bash]
----
helm lint argocd-config/
helm package argocd-config/
helm repo index --url https://alvarolop.github.io/ocp-gitops-playground/ .
----

And then push the changes to the repo.

== Installing ArgoCD and Openshift GitOps

https://docs.openshift.com/gitops/1.10/understanding_openshift_gitops/about-redhat-openshift-gitops.html[Red Hat OpenShift GitOps] Red Hat OpenShift GitOps uses https://argo-cd.readthedocs.io/en/stable[Argo CD] to manage specific cluster-scoped resources, including platform operators, optional Operator Lifecycle Manager (OLM) operators, and user management. Argo CD is a popular Cloud Native Computing Foundation (CNCF) open-source GitOps Kubernetes Operator for declarative configuration on Kubernetes clusters. 

=== The GitOps Operator

First, let's install the OpenShift GitOps Operator on OCP and log in to the Argo CD instance.

[source, bash]
----
oc process -f openshift/01-operator.yaml | oc apply -f -
----

NOTE: After the installation is completed, the operator pods will be running in the `openshift-operators` project.

By default, the GitOps operator deploys an ArgoCD instance in the `openshift-gitops` project. To avoid that and have full control of the installation location, the https://access.redhat.com/solutions/6097231[following configuration] has been set in the previous OCP template.

[source, yaml]
----
    spec:
      config:
        env:
          - name: DISABLE_DEFAULT_ARGOCD_INSTANCE
            value: "true"
          - name: DISABLE_DEFAULT_ARGOCD_CONSOLELINK
            value: "true"
----

=== The GitOps Instance

Second, we are going to deploy our ArgoCD instance manually using the following template:

[source, bash]
----
oc process -f openshift/02-argocd.yaml | oc apply -f -
----

This command will create a namespace `{ocp-cluster-project}`, a default Application Project and an ArgoCD cluster. The configuration used is the default Operator configuration with some small improvements: Delegating authentication on the OpenShift integrated login, and implementing basic RBAC policies.

Access the installed ArgoCD cluster using the following route:

[source, bash, subs="attributes"]
----
oc get routes {ocp-cluster-name}-server -n {ocp-cluster-project} --template="https://{{.spec.host}}"
----



=== Configuring Authentication

NOTE: After the Red Hat OpenShift GitOps Operator is installed, Argo CD automatically creates a user with admin permissions. However, we have disabled Admin access in the ArgoCD RBAC using the property `.spec.disableAdmin`. 

Dex is installed by default for all the Argo CD instances created by the Operator. You can configure Red Hat OpenShift GitOps to use Dex as the SSO authentication provider by setting the .spec.sso parameter. This is the current configuration:

[source, yaml]
----
spec:
  sso:
    dex:
      openShiftOAuth: true
    provider: dex
----

Dex uses the users and groups defined within the OpenShift Container Platform by checking the OAuth server provided by the platform. Use the following configuration so that every OCP `cluster-admin` is an ArgoCD admin:

[source, yaml]
----
spec:
  rbac:
    defaultPolicy: 'role:admin'
    policy: |
      g, system:cluster-admins, role:admin
      g, cluster-admins, role:admin
    scopes: '[groups]'
----





=== Configuring authorization

CAUTION: Authorization in ArgoCD is a combination of configuring permissions at different levels. Here we present all three levels that you have to take care of to set proper authorization configuration. Please, read the three following sections carefully. 


==== ArgoCD managed namespaces

Openshift GitOps supports several ArgoCD clusters in the same OCP. This feature is essential for organizations that want to implement full multi-tenancy at the ArgoCD cluster level. With this feature available, now we have to distribute each namespace among the ArgoCD clusters. This is done using the `argocd.argoproj.io/managed-by` label.

Add a label to the application's namespace so that the Argo CD instance in the `{ocp-cluster-project}` namespace can manage it:

[source, bash, subs="attributes"]
----
oc label namespace {ocp-app-project} argocd.argoproj.io/managed-by={ocp-cluster-project}
----

If you don't do so, the error message that you will see in the web console when you try to synchronize an application is:

[source, bash]
----
Namespace "<namespace>" for <resource> "<resource-name>" is not managed.
----

Link to the https://docs.openshift.com/gitops/1.10/declarative_clusterconfig/configuring-an-openshift-cluster-by-deploying-an-application-with-cluster-configurations.html#creating-an-application-by-using-the-oc-tool_configuring-an-openshift-cluster-by-deploying-an-application-with-cluster-configurations[documentation]. 


==== Managing cluster-wide resources

Cluster resources are not bound to a namespace, and, therefore, are not affected by the previous label. For that reason, non-default ArgoCD instances cannot control them. If you want to do so, you need to instruct the GitOps operator to allow it for your cluster like in the following example:

[source, yaml]
----
    spec:
      config:
        env:
          - name: ARGOCD_CLUSTER_CONFIG_NAMESPACES
            value: openshift-gitops, gitops
----




==== Using namespace Isolation at Service Account level

The ArgoCD instance only has privileges in its namespace which is `{ocp-cluster-project}`. For creating/updating/listing resources in other namespaces, it's mandatory to update the RBAC for its Service Account.

This section can be as complex as the security requirements that your organization demands for the ArgoCD deployment. The easiest solution for non-productive environments would be to grant `cluster-admin` rights to the service account that interacts with the k8s API.

[source, bash, subs="attributes"]
----
oc adm policy add-cluster-role-to-user admin system:serviceaccount:{ocp-cluster-project}:{ocp-cluster-name}-argocd-application-controller
----


If you prefer to have a per-project tunning, you can use the configuration set in the template `openshift/11-application-app.yaml`, where we provide project admin rights to the SA. This is also oriented to get a proper multi-tenancy configuration, like in the previous section. Check the template mentioned or use the following command:

[source, bash, subs="attributes"]
----
oc adm policy add-role-to-user admin system:serviceaccount:{ocp-cluster-project}:{ocp-cluster-name}-argocd-application-controller -n {ocp-app-project}
----


Obviously, you can even set a finer tunning by creating a custom `Role` and `RoleBinding` to specify the resources that each ArgoCD will be allowed to manage per namespace. This https://access.redhat.com/solutions/5875661[KCS] gives you an example of how to configure one of these `RoleBindings`.



Extra documentation:

* https://blog.andyserver.com/2020/12/argocd-namespace-isolation[Deep-dive blog post] about namespace isolation using the SA `RoleBindings`.
* https://github.com/redhat-developer/gitops-operator/issues/116[Upstream issue] regarding permissions for the ArgoCD instance.











== Annex: Installing Nexus

[IMPORTANT]
====
TL;DR: Execute the following script to auto-install a Nexus instance in your cluster:

[source, bash]
----
./nexus-auto-install.sh
----
====

Nexus Repository OSS is an open-source repository that supports many artifact formats, including Docker, Java™, and npm. With the Nexus tool integration, pipelines in your toolchain can publish and retrieve versioned apps and their dependencies by using central repositories that are accessible from other environments.

If you are planning to deploy your applications using Helm charts, most of the architectures you will need a Helm repository to host packaged Helm charts. Install a Nexus repository manager using the following commands:


[source, bash]
----
# Define common variables
OPERATOR_NAMESPACE="nexus"

# Deploy operator
oc process -f openshift/nexus/01-operator.yaml \
  -p OPERATOR_NAMESPACE=$OPERATOR_NAMESPACE | oc apply -f -

# Deploy application instance
oc process -f openshift/nexus/02-server.yaml \
  -p OPERATOR_NAMESPACE=$OPERATOR_NAMESPACE \
  -p SERVER_NAME="nexus-server" | oc apply -f -
----

=== Creating  a Helm repository

Create a Helm repository with the following steps:

* Access the Nexus route: `oc get routes nexus-server --template="https://{{.spec.host}}"`.
* Log in using the admin credentials: `admin` / `admin123`.
* Server Administration > Repositories > Create Repositories > "Helm(hosted)"
** name: `helm-charts`.
** DeploymentPolicy: `Allow redeploy`.
* Click on `Create repository`.

NOTE: If you don't want to use the console, you can use the `curl` command to create this repository. Check an example in the `auto-install-nexus.sh` script.
